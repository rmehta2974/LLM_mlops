apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-llm-v3
  namespace: llm-inference
spec:
  replicas: 2
  selector:
    matchLabels:
      app: triton-llm
      version: v3
  template:
    metadata:
      labels:
        app: triton-llm
        version: v3
    spec:
      nodeSelector:
        node-role.kubernetes.io/gpu: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: triton
        image: registry.example.com/llm/triton-llama3-70b:v3
        args: ["tritonserver","--model-repository=/models"]
        ports:
        - name: http
          containerPort: 8000
        - name: grpc
          containerPort: 8001
        - name: metrics
          containerPort: 8002
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        readinessProbe:
          httpGet:
            path: /v2/health/ready
            port: 8000
          initialDelaySeconds: 20
          periodSeconds: 10
